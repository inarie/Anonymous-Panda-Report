\section{The Anonymous Face and Hand Tracking Panda}
When reviewing and testing the Anonymous Face Tracking Panda prototype, we found that the virtual avatar's ability to express themselves was limited by the lack of hand movement, which made it seem unrealistic and similar to a puppet acting. Through this finding, we questioned if the addition of hand tracking would affect or not the ability to elicit the same emotional response as human-to-human interactions (\textbf{RQ\textsubscript{1}}), and if it would help increase interactants' verbal self-disclosure or not (\textbf{RQ\textsubscript{2}}).

When people speak, they frequently make spontaneous hand movements that occur in synchrony with speech and naturally accompany all spoken conversation; such movements are known as gestures \cite{CLO20}. Gestures have a variety of roles in communication, learning, and comprehension for both those who observe them and those who make them. Gestures are especially effective when they resemble the thought they express, which gives them an advantage over words since it adds meaningful and distinct information while reflecting the speaker's underlying knowledge and experiences \cite{CLO20, KAN16B}. According to Feyereisen and de Lannoy \cite{FEY91}, people from all known cultures and language backgrounds gesture. Hence gestures are essential for making connections with others, although styles vary across countries.

Hand gestures, which combined with facial expressions and articulated words, play an important role in conveying our emotions. To create emotional depiction, hand movements usually combine nontrivially with facial expressions \cite{ARJ20}. It is commonly known that a person who utilizes his hands powerfully while sharing his views attracts significantly more attention than a person who only uses his voice to deliver an idea \cite{COO10, WAK18}.

As previously stated, the primary purpose of this thesis is to create a functional prototype capable of connecting people with mental health specialists anonymously, while still maintaining patient-therapist empathy. Therefore the addition of hand tracking is a critical step toward our goal in order to get deeper emotional recognition. Because, in addition to facial emotions and body posture, hands are the most significant nonverbal clues to identify specific states in others, such as anxiety states \cite{WAX97, REI22}. 

Some studies suggest that nonverbal cues may be more reliable indicators of clinical conditions than patient verbal information \cite{KNA13}. For example, research in clinical psychology has discovered that human patients' nonverbal behavior unintentionally revealed intimate information that was not given in their spoken behavior \cite{FAB06, KLE03}. Nonverbal behavior, in general, plays an important part in the establishment and maintenance of a therapeutic relationship through establishing rapport between therapists and patients in psychotherapy interactions \cite{KLE03} â€” in other words, therapists dedicate significant time and effort to carefully following patients' nonverbal behavior and adjusting their own nonverbal conduct in order to respond effectively and develop trust \cite{ABA21}. Furthermore, according to Abargil et al. \cite{ABA21}, through the recognition of emotions, therapists are able to adjust therapy according to the patient's nonverbal and verbal cues, and thus obtain a better outcome. Another example reveals that when describing pain, co-speech gestures convey additional information beyond that provided in speech, potentially making an essential contribution to the communication of this experience and providing hints for changing the therapy methods and results \cite{ROW16, REI22}.

\subsection{Approach}
As previously mentioned, various researchers believe that hand gestures, in conjunction with facial expressions and articulated words, play a significant part in conveying our emotions. As a result, adding hand tracking to the current prototype is a vital step in achieving our goal of deeper emotional recognition. Because, aside from facial expressions and body posture, hands are the most important nonverbal cues for recognizing specific states in others \cite{WAX97, REI22}.

While there are a variety of ways to perform hand tracking, such as with virtual reality devices or gloves, we believe that the Leap Motion Controller is the best option for this thesis because it is a practical and small device that does not interfere with the face recognition part.

\subsubsection{Leap Motion Controller and Unreal Plugin}
The Leap Motion Controller (Figure \ref{fig:exampleLeap}) is a device that connects to a PC or a Mac and allows users to manipulate digital objects using hand motions. It adds a new way to interact with the digital world when combined with other hardware. Programs that interpret gesture-based computing allow users to play games, design, and learn in a hands-on manner. This device maps and tracks the human hand using an infrared scanner and sensor. This data is used to create a digital version of the hand that can manipulate digital objects in real time.

\begin{figure}[!htb]
\includegraphics[width=0.6\textwidth]{figures/leapMotion.jpg}
\centering
\caption{Example of the Leap Motion tracking}
\label{fig:exampleLeap}
\end{figure}

In order to connect the existing Anonymous Panda prototype with the hand tracking part, it was necessary to use the official UE4 plugin, the Ultraleap Hand Tracking Plugin \cite{ULT}. This plugin allows Unreal developers to make use of the data obtained by incorporating Ultraleap's hand tracking data into their projects. It was designed with the intention of developing and implementing hand tracking in Extended Reality (XR) projects \cite{XR}, but with the right adjustments, it was possible to adapt its use to other types of projects \cite{ULTG}.

As mentioned, the Leap Motion Controller tracks hands and fingers with low latency and high precision, reporting position, velocity, and orientation. This controller may be placed on a VR headset or used on a tabletop. To incorporate Ultraleap's hand tracking data into a project, the Leap motion Controller system, which runs as a service or daemon, analyzes the images produced by its hardware and sends the analyzed data to the project. Then, the unreal plugin connected to the mentioned service converts the received data to the Unreal coordinate system. Furthermore, the plugin automatically transforms the tracking data to use a left-handed coordinate system and scales distance values to centimeters because the UE uses a left-handed convention for its coordinate system and centimeters as the default unit, whereas the Leap Motion uses a right-handed convention and millimeters as the default unit \cite{ULTP}. Figure \ref{fig:unrealAxes} shows the Unreal coordinate system in relationship to the Leap Motion device in Head Mounted Display (HMD) mode.

\begin{figure}[!htb]
\includegraphics[width=0.3\textwidth]{figures/unrealAxes.png}
\centering
\caption{The Unreal coordinate system in relationship to the Leap Motion device in HMD mode}
\label{fig:unrealAxes}
\end{figure}

When the Leap Motion Controller has a good, high-contrast view of an object's silhouette, it uses infrared light and optical sensors to perform the detection and tracking work. These sensors have a $150^\circ$ field of view and their effective range extends from approximately 3 to 60 centimetres (Figure \ref{fig:HMDistance}). Last but not least, the Leap Motion software combines sensor data with an internal model of the human hand to handle complex tracking conditions \cite{ULTP}.

\begin{figure}[!htb]
\includegraphics[width=0.4\textwidth]{figures/HMDistance.png}
\centering
\caption{In HMD mode, there should be less distance between the sensor and the hands than between the hands and any background objects or walls}
\label{fig:HMDistance}
\end{figure}

\subsubsection{The Hand Tracking Panda}
Because it is a useful, compact device that does not obstruct the face tracking component, we think the Leap Motion Controller is the ideal choice for this work. Therefore, in order to have the best performance in hand tracking, we chose to use the Leap Motion Controller in HMD mode because the Ultraleap Hand Tracking Plugin rotates the 3D hands to keep the correct orientation to the Unreal world when this mode is active \cite{ULTP}. In contrast with the Desktop Mounted Display (DMD) mode (Figure \ref{fig:DMD}), we can see that hand orientation in DMD mode is not ideal for our work since we want to track gestures, which are primarily vertical and happen while individuals are communicating with one another. This concern could be resolved by rotating the data gathered by the Leap Motion Controller before it is applied in the hands of the metahuman, however doing so would cause the prototype to perform slightly slower. We also decided to position the Leap Motion on the chest using a shirt clip made by a 3D printer, as shown in Figure \ref{fig:CMDvsHMD} (b) because using it on a VR support would obstruct the face tracking functionality (imagine having a VR headset as Figure \ref{fig:CMDvsHMD} (a) and wanting the metahuman to blink its right eye).

\begin{figure}[!htb]
\includegraphics[width=0.6\textwidth]{figures/LeapView.jpg}
\centering
\caption{View of hands in DMD mode}
\label{fig:DMD}
\end{figure}

\begin{table}[!htb]
    \begin{minipage}{\linewidth}
        \centering
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=\linewidth]{figures/VRsupport.png}
            \caption{Head (VR support)}
        \end{subfigure}
        \begin{subfigure}{0.49\textwidth}
            \includegraphics[width=\linewidth]{figures/CMD.jpg}
            \centering
            \caption{Chest}
        \end{subfigure}
        \captionof{figure}{Head vs. Chest Mounted Mode}
        \label{fig:CMDvsHMD}
	\end{minipage}
\end{table}

The official Ultraleap Tracking Plugin for Unreal Engine 4 utilizes pre-made blueprints to add hand tracking to a scene and play it in the editor. However, depending on the project, it may take a more customized approach using C\texttt{+}\texttt{+} or blueprints \cite{ULTGIT}. For our project, we attempted to adapt a custom rigging example map from the Ultraleap Plugin, which contained several animation blueprints \cite{LEAPMOD}, to create the new version of the Anonymous Panda prototype, beginning by seeing how each rigging was done and how we could adapt it to our metahumans' bones.

To use the rigging on our metahumans effectively, we had to re-parent the metahuman animation blueprint to "BodyStateAnimInstance". By utilizing the Body State system, we were able to map the tracked data collected by the Leap Motion Controller to the metahumans' skeletal mesh bones (Figure \ref{fig:mappedBones}).

\begin{table}[!htb]
    \begin{minipage}{\linewidth}
        \centering
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{figures/mappedBones.png}
            \caption{Left Side}
        \end{subfigure}
        \begin{subfigure}{0.49\textwidth}
            \includegraphics[width=\textwidth]{figures/mappedBonesR.png}
            \centering
            \caption{Right Side}
        \end{subfigure}
        \captionof{figure}{Tracked data mapped to the metahumans' skeletal hands mesh bones}
        \label{fig:mappedBones}
	\end{minipage}
\end{table}

We used the two Mapped Bone Anim Data arrays that were previously seen (Figure \ref{fig:mappedBones}), much like the two-handed example from the custom rigging map, and chose to disable the deformation mesh because the metahuman model does not yet allow deformation (Figure \ref{fig:mappedBoneList}).

\begin{figure}[!htb]
\includegraphics[width=0.8\textwidth]{figures/mappedBoneList.png}
\centering
\caption{Mapped Bone List}
\label{fig:mappedBoneList}
\end{figure}

Since we disabled the deformation mesh, only the rotations were tracked. To move the elbows to their proper positions, we apply an animation node called Forward And Backward Reaching Inverse Kinematics (FABRIK) to each elbow. The FABRIK animation node is an Inverse Kinematics (IK) solver that handles joint rotation from the location of an end-effector rather than directly from the joint rotation. In practice, we identify an effector location, and the IK solution solves the rotation so that the final joint is as close to that location as possible \cite{IK}.

Most animated skeletons in UE are powered by direct rotational data provided directly into the character's bones or the Skeletal Mesh. This is called Forward Kinematics (FK), or the direct application of rotation to joints or bones. Figure \ref{fig:diagramFK} illustrates a diagram of the concept.

\begin{figure}[!htb]
\includegraphics[width=0.8\textwidth]{figures/diagram_FK.png}
\centering
\caption{Forward Kinematics}
\label{fig:diagramFK}
\end{figure}

Meanwhile, Inverse Kinematics operates in the opposite direction. Instead of rotating bones, we provide the bone chain a target (also known as an end effector), specifying a position that the chain's end should aim for. The effector is moved by the user, and the IK solver (the algorithm that drives rotation in an IK system) rotates the bones so that the final bone in the chain ends at the target location \cite{IK}. The red cross in Figure \ref{fig:diagramIK} represents the end effector.

\begin{figure}[!htb]
\includegraphics[width=0.8\textwidth]{figures/diagramIK.png}
\centering
\caption{Inverse Kinematics}
\label{fig:diagramIK}
\end{figure}

Once again, in order to position the shoulders correctly, we had to rotate all of our mapped data by 90 degrees in the "Offset Transform" for Y, as is typical for skeletal meshes and as shown in Figure \ref{fig:mappedBoneList}.

Concerning the prototype's outcome, Figure \ref{fig:BPHandsAndIK} depicts the steps taken from providing the IK with all of the data required for its operation to the metahuman's animation in the blueprint, after gathering data for the hands animation. Finally, Figure \ref{fig:handTrack} and videos \cite{APT1,APT2} illustrate the outcome applied to one of the metahumans used in this study.

\begin{figure}[!htb]
\includegraphics[width=\textwidth]{figures/BPHandsAndIK.png}
\centering
\caption{Part of the blueprint in control of animating metahuman hands using data collected by leap motion}
\label{fig:BPHandsAndIK}
\end{figure}

\begin{figure}[!htb]
\includegraphics[width=0.5\textwidth]{figures/final.png}
\centering
\caption{The hand tracking's final outcome}
\label{fig:handTrack}
\end{figure}

\paragraph{Challenges}
The exploration of the ultraleap plugin was one of the first challenges encountered during the development of this version of the prototype. Despite being documented, the examples provided were difficult to replicate due to the limited amount of online content that could help with this issue. We tried using github issues to ask for help since the solution was still far from being found but were unsuccessful. We eventually addressed the problem by downgrading the plugin version to version 4.2.0. 

After we were able to successfully reproduce the plugin demonstrations, we made the decision to investigate hand tracking using the rigging method. Following the example of the custom rigging map, we adapted some of its content to our animation blueprints, but we were unable to achieve the same effect as the rigging example. Our initial attempt is shown in Figure \ref{fig:initialSteps} (a), which only shows the left arm, and Figure \ref{fig:initialSteps} (b), which shows both arms. The shoulders and hands in both figures have a significant deformation, but they were still capable of movement.

\begin{table}[!htb]
    \begin{minipage}{\linewidth}
        \centering
        \begin{subfigure}{0.49\textwidth}
            \centering
            \includegraphics[width=0.5\textwidth]{figures/issue.png}
            \caption{Hand and Shoulder deformation}
        \end{subfigure}
        \begin{subfigure}{0.49\textwidth}
            \includegraphics[width=0.68\textwidth]{figures/FirstInteraction.png}
            \centering
            \caption{Hands and Shoulders deformations}
        \end{subfigure}
        \captionof{figure}{Initial Hand Tracking Steps}
        \label{fig:initialSteps}
	\end{minipage}
\end{table}

The leap motion controller broke while we were trying to solve this issue, and we had to wait 3 weeks for a replacement. Since this device was absolutely necessary for the project, we used these three weeks to search for solutions online, in the plugin's documentation, and in github issues. In the end, we discovered a video on YouTube featuring a metahuman who could move his arms using the hand tracking data from leap motion \cite{TVL}. After contacting the person who uploaded the video, we were able to get them to provide some advice and tips about how hand tracking should be used properly in Unreal Engine. We made an effort to stick to the advice provided, but we were unable to achieve the desired outcome. The result, in which the arms were infinitely stretched, is shown in Figure \ref{fig:armsStretching}.

\begin{figure}[!htb]
\includegraphics[width=\textwidth]{figures/handStretching.png}
\centering
\caption{Left arm stretching infinitely}
\label{fig:armsStretching}
\end{figure}

We got in touch with the person from the video again, and he gave us additional advice on how to handle this. The result can be observed in Figure \ref{fig:closeOutcome} and we can notice that the outcame is getting closer to what we wanted (Figure \ref{fig:handTrack}). Still, in order to achieve a more realistic effect, we must elevate our metahumans' hands to waist level.

\begin{figure}[!htb]
\includegraphics[width=\textwidth]{figures/handMinPosition.png}
\centering
\caption{One step closer to the final outcome}
\label{fig:closeOutcome}
\end{figure}

To offset the metahumans' hands and raise them to waist height, we followed the advice of the individual who uploaded the YouTube video and added a positive constant on the Y-axis to the "Effector Transform Location" vector before the IK solver performed its magic. Although the output was closer to what was intended, the fingers stopped functioning normally (Figure \ref{fig:fingersIssue} and video \cite{APT3}).

\begin{figure}[!htb]
\includegraphics[width=0.5\textwidth]{figures/otherIssue.png}
\centering
\caption{Issues with the fingers as the outcome neared}
\label{fig:fingersIssue}
\end{figure}

While trying to figure out how to get the fingers to work properly again, we received a new tip to not map the metacarpal bones of the fingers, which solved our problem. However, the arms started to stretch out indefinitely once more (Figure \ref{fig:armsStretching}). To solve this issue, we noticed that mapping the lower arm to none solved the problem, and thus we obtained our final result (Figure \ref{fig:handTrack}).

\paragraph{Limitations}
As was already noted, many studies think that hand gestures, along with facial expressions and spoken words, are important in expressing our emotions. In order to achieve our goal of deeper emotional recognition, including hand tracking to the existing prototype was a crucial step. Because hands are the most crucial nonverbal clues for identifying particular states in others, after facial expressions and body position \cite{WAX97, REI22}.

There are a number of ways to accomplish hand tracking, such as with virtual reality equipment or gloves, but we felt that the Leap Motion Controller was the ideal choice for this work because it is a useful and compact device that does not interfere with the face recognition component. This device, however, has significant limitations. There is a reliance on the usage of the USB cable for leap motion and unreal communication, and because it is an old device, acquiring it may be difficult. Additionally, infrared sensors may not function properly in bright environments, and turning on the HMD mode requires the Ultraleap Visualizer program rather than the blueprint configuration because doing so interferes with the hand tracking data.

Last but not least, a 3D-printed dependency of two shirt clips is shown in Figure \ref{fig:CMDvsHMD} (b). In the beginning, only one shirt clip was required, however throughout the prototype tests, instability and imprecise hand tracking capture were detected. Although the two clips are currently functional, a more stable and user-friendly solution would be preferable.

\subsubsection{Anonymous Panda User Interface}
There was some feedback from friends and family on the prototype interface during several project demonstrations. Much of this feedback was connected to the lack of other metahumans, which were present but the opportunity to switch between them was not easily identified, and users were always forced to choose between the initial two metahumans since they were unaware of this option.
In response to these critiques, we made an effort to develop a new user interface that was simpler to use and made all of the prototype's metahumans available at the time of user selection (Figure \ref{fig:newUI}). Since the prior study did not allow us to test the user interface with actual users, we were unable to identify this issue which could have been addressed earlier. We anticipate to get further feedback on this problem in the upcoming study by testing the User Interface (UI) with actual users and, if necessary, changing it.

\begin{figure}[!htb]
\includegraphics[width=\textwidth]{figures/startMenu.png}
\includegraphics[width=\textwidth]{figures/metahumanMenu.png}
\centering
\caption{New Anonymous Panda UI}
\label{fig:newUI}
\end{figure}

\subsection{Research Model and Hypotheses}

\subsubsection{Technology Acceptance Model}
Our goal in this research is to put to the test an extended Technology Acceptance Model (TAM) designed for studying the user acceptance of our work, the Anonymous Panda.

The TAM predicts user intention to utilize a given technology based on two user perceptions: perceived utility and perceived ease of use. Perceived usefulness is defined as "the degree to which a person believes that using a certain system would improve his or her job performance" \cite{DAV89}. Perceived ease of use is defined as "the degree to which a person believes that using a certain system would be devoid of effort" \cite{DAV89}.

The Technology Acceptance Model is a practical model that can be used in a variety of situations, and it has been expanded through numerous studies to accommodate different technologies, situations, and users. Features from similar models (e.g., subjective norm, perceived behavioral control), new belief factors (e.g., triability, content richness), and external variables (e.g., demographic characteristics, computer self-efficacy) have all been proposed as TAM adaptations. External variables can be antecedents or moderators of perceived usefulness and perceived usability. These extensions aim to improve the TAM's ability to predict outcomes for specific technologies, contexts, and users by adding additional variables \cite{CAM20}.

Several studies that have used the TAM to examine a person's intention to use certain technologies have demonstrated that the TAM can be useful in a variety of contexts \cite{CAM20}. In addition, they discovered that perceived usefulness and perceived ease of use predicted intention to use. And finally, discovered a significant relationship between perceived ease of use and perceived usefulness. Therefore, we propose the three following hypotheses:

\textbf{\textit{H}\textsubscript{1}} - Perceived ease of use has a positive effect on the intention to use Anonymous Panda.

\textbf{\textit{H}\textsubscript{2}} - Perceived usefulness has a positive effect on the intention to use Anonymous Panda.

\textbf{\textit{H}\textsubscript{3}} - Perceived ease of use has a positive effect on perceived
usefulness.

\subsubsection{Perceived Anonymity}


\subsubsection{Online Public Disclosure}


\subsubsection{Research Model}
Our research model is summarized in Figure X, which also shows the hypothesized relationships between the variables.

\subsection{Methodology}


\subsubsection{Participants}


\subsubsection{Procedure}


\subsubsection{Statistical Analyses}


\subsubsection{Results}


\subsubsection{Discussion}

